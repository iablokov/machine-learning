{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to train a GNN for the inorder traversal of a binary tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import pickle, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"balanced-shuffled\"\n",
    "path_dataset = f\"./data/{dataset_name}\"\n",
    "path_dataset_pkl = f\"{path_dataset}/dataset.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "with open(path_dataset_pkl, \"rb\") as f : dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F \n",
    "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features  : 1\n",
      "Output features : 10\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 32\n",
    "seed = 42\n",
    "inp_features = 1\n",
    "out_features = dataset[0].y.shape[0]\n",
    "\n",
    "print(f\"Input features  : {inp_features}\")\n",
    "print(f\"Output features : {out_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (initial_conv): GCNConv(1, 32)\n",
      "  (conv1): GCNConv(32, 32)\n",
      "  (conv2): GCNConv(32, 32)\n",
      "  (conv3): GCNConv(32, 32)\n",
      "  (out): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Number of parameters:  3882\n"
     ]
    }
   ],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # Init parent\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # GCN layers\n",
    "        self.initial_conv = GCNConv(inp_features, embedding_size)\n",
    "        self.conv1 = GCNConv(embedding_size, embedding_size)\n",
    "        self.conv2 = GCNConv(embedding_size, embedding_size)\n",
    "        self.conv3 = GCNConv(embedding_size, embedding_size)\n",
    "\n",
    "        # Output layer\n",
    "        self.out = Linear(embedding_size*2, out_features)\n",
    "\n",
    "    def forward(self, x, edge_index, batch_index):\n",
    "        # First Conv layer\n",
    "        hidden = self.initial_conv(x, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "\n",
    "        # Other Conv layers\n",
    "        hidden = self.conv1(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "        hidden = self.conv2(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "        hidden = self.conv3(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "          \n",
    "        # Global Pooling (stack different aggregations)\n",
    "        hidden = torch.cat([gmp(hidden, batch_index), \n",
    "                            gap(hidden, batch_index)], dim=1)\n",
    "\n",
    "        # Apply a final (linear) classifier.\n",
    "        out = self.out(hidden)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "model = GCN()\n",
    "print(model)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(torch.cuda.is_available(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(dataset)\n",
    "batch_size = 64\n",
    "trn_loader = DataLoader(dataset[:int(dataset_size * 0.8)], batch_size=batch_size, shuffle=True)\n",
    "tst_loader = DataLoader(dataset[int(dataset_size * 0.8):], batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader, device):\n",
    "    for batch in loader:\n",
    "        batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        print(batch.x.shape, batch.edge_index.shape, batch.batch.shape)\n",
    "        pred, embedding = model(batch.x.float(), batch.edge_index, batch.batch)\n",
    "        loss = loss_fn(pred, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "torch.Size([640, 1]) torch.Size([9, 128]) torch.Size([640])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 9 but got size 2 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2000\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     loss, h \u001b[39m=\u001b[39m train(trn_loader, device)\n\u001b[0;32m      5\u001b[0m     losses\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m      6\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[24], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(loader, device)\u001b[0m\n\u001b[0;32m      4\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(batch\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mshape, batch\u001b[39m.\u001b[39medge_index\u001b[39m.\u001b[39mshape, batch\u001b[39m.\u001b[39mbatch\u001b[39m.\u001b[39mshape)\n\u001b[1;32m----> 6\u001b[0m pred, embedding \u001b[39m=\u001b[39m model(batch\u001b[39m.\u001b[39;49mx\u001b[39m.\u001b[39;49mfloat(), batch\u001b[39m.\u001b[39;49medge_index, batch\u001b[39m.\u001b[39;49mbatch)\n\u001b[0;32m      7\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred, batch\u001b[39m.\u001b[39my)\n\u001b[0;32m      8\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\X\\.conda\\envs\\ml-pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[18], line 18\u001b[0m, in \u001b[0;36mGCN.forward\u001b[1;34m(self, x, edge_index, batch_index)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, edge_index, batch_index):\n\u001b[0;32m     17\u001b[0m     \u001b[39m# First Conv layer\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minitial_conv(x, edge_index)\n\u001b[0;32m     19\u001b[0m     hidden \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mtanh(hidden)\n\u001b[0;32m     21\u001b[0m     \u001b[39m# Other Conv layers\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\X\\.conda\\envs\\ml-pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\X\\.conda\\envs\\ml-pytorch\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:210\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    208\u001b[0m cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index\n\u001b[0;32m    209\u001b[0m \u001b[39mif\u001b[39;00m cache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 210\u001b[0m     edge_index, edge_weight \u001b[39m=\u001b[39m gcn_norm(  \u001b[39m# yapf: disable\u001b[39;49;00m\n\u001b[0;32m    211\u001b[0m         edge_index, edge_weight, x\u001b[39m.\u001b[39;49msize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim),\n\u001b[0;32m    212\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimproved, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_self_loops, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflow, x\u001b[39m.\u001b[39;49mdtype)\n\u001b[0;32m    213\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached:\n\u001b[0;32m    214\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index \u001b[39m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[1;32mc:\\Users\\X\\.conda\\envs\\ml-pytorch\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:91\u001b[0m, in \u001b[0;36mgcn_norm\u001b[1;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[0;32m     88\u001b[0m num_nodes \u001b[39m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m add_self_loops:\n\u001b[1;32m---> 91\u001b[0m     edge_index, edge_weight \u001b[39m=\u001b[39m add_remaining_self_loops(\n\u001b[0;32m     92\u001b[0m         edge_index, edge_weight, fill_value, num_nodes)\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m edge_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     edge_weight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((edge_index\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), ), dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m     96\u001b[0m                              device\u001b[39m=\u001b[39medge_index\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\X\\.conda\\envs\\ml-pytorch\\Lib\\site-packages\\torch_geometric\\utils\\loop.py:370\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[1;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[0;32m    366\u001b[0m     loop_attr[edge_index[\u001b[39m0\u001b[39m][inv_mask]] \u001b[39m=\u001b[39m edge_attr[inv_mask]\n\u001b[0;32m    368\u001b[0m     edge_attr \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([edge_attr[mask], loop_attr], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m--> 370\u001b[0m edge_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([edge_index[:, mask], loop_index], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    371\u001b[0m \u001b[39mreturn\u001b[39;00m edge_index, edge_attr\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 9 but got size 2 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "losses = []\n",
    "for epoch in range(2000):\n",
    "    loss, h = train(trn_loader, device)\n",
    "    losses.append(loss)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch} | Train Loss {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
